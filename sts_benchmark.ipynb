{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import en_core_web_lg\n",
    "import wordfreq\n",
    "\n",
    "from embedding_vectorizer import EmbeddingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STS_BENCHMARK_DIR = './data/stsbenchmark/'\n",
    "WORD_FREQUENCIES_FILE = './data/enwiki_vocab_min200.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sts_benchmark(directory):\n",
    "    def load_dataset(directory, file):\n",
    "        dataset = []\n",
    "        with open(os.path.join(directory, file), 'r') as tsvfile:\n",
    "            reader = csv.reader(tsvfile, delimiter='\\t', quotechar='|')\n",
    "            for row in reader:\n",
    "                score = row[4]\n",
    "                sent1 = row[5]\n",
    "                sent2 = row[6]\n",
    "                dataset.append((sent1, sent2, float(score)))\n",
    "        return dataset\n",
    "    \n",
    "    datasets = map(lambda split: load_dataset(directory, 'sts-{}.csv'.format(split)), ['train', 'dev', 'test'])\n",
    "    return tuple(datasets)\n",
    "\n",
    "def load_word_frequencies(path):\n",
    "    word_freq = {}\n",
    "    N = 0\n",
    "    with open(path, 'r') as freq_file:\n",
    "        reader = csv.reader(freq_file, delimiter=' ', quotechar='|')\n",
    "        for row in reader:\n",
    "            word = row[0]\n",
    "            freq = int(row[1])\n",
    "            word_freq[word] = freq\n",
    "            N += freq\n",
    "    return {k: v / N for k, v in word_freq.items()}\n",
    "\n",
    "def dataset_iter(dataset, yield_s1=False, yield_s2=False):\n",
    "    for sent1, sent2, _ in dataset:\n",
    "        if yield_s1:\n",
    "            yield sent1\n",
    "        if yield_s2:\n",
    "            yield sent2\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            \n",
    "def similarity_scores(dataset, model):\n",
    "    scores = []\n",
    "    for s1, s2, _ in dataset:\n",
    "        score = cosine_similarity(model.transform([s1])[0], model.transform([s2])[0])\n",
    "        scores.append(score)\n",
    "    return np.array(scores)\n",
    "\n",
    "def pearson_r(true_scores, pred_scores):\n",
    "    from scipy.stats import pearsonr\n",
    "    return pearsonr(true_scores, pred_scores)[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = load_sts_benchmark(STS_BENCHMARK_DIR)\n",
    "word_freq = load_word_frequencies(WORD_FREQUENCIES_FILE)\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vectorizer(train, test, word_vectorizer, tokenizer=None, word_freq=None,\n",
    "                        weighted=True, remove_components=1, lowercase=True):\n",
    "    \n",
    "    print('Samples in training dataset:', len(train))\n",
    "    print('Samples in test dataset', len(test))\n",
    "\n",
    "    emb_vectorizer = EmbeddingVectorizer(\n",
    "        tokenizer=tokenizer,\n",
    "        word_vectorizer=word_vectorizer,\n",
    "        word_freq=word_freq,\n",
    "        weighted=weighted,\n",
    "        remove_components=remove_components,\n",
    "        lowercase=lowercase)\n",
    "    \n",
    "    print('Fitting on training dataset')\n",
    "    emb_vectorizer.fit(dataset_iter(train, yield_s1=True, yield_s2=True))\n",
    "\n",
    "    print('Computing similarity scores on test dataset')\n",
    "    scores_pred = similarity_scores(test, emb_vectorizer)\n",
    "    scores_true = [d[2] for d in test]\n",
    "\n",
    "    print('Pearson`s r (x 100) on test dataset:', pearson_r(scores_true, scores_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in training dataset: 5749\n",
      "Samples in test dataset 1500\n",
      "Fitting on training dataset\n",
      "Computing similarity scores on test dataset\n",
      "Pearson`s r (x 100) on test dataset: 80.2393160533\n"
     ]
    }
   ],
   "source": [
    "evaluate_vectorizer(train=train, test=dev,\n",
    "                    word_vectorizer=lambda w: nlp.vocab[w].vector,\n",
    "                    tokenizer=lambda s: [t.text for t in nlp.tokenizer(s) if not t.is_punct and not t.is_space],\n",
    "                    word_freq=lambda w: word_freq.get(w, 0.0),\n",
    "                    # word_freq=lambda w: wordfreq.word_frequency(w, 'en', wordlist='large'),\n",
    "                    weighted=True,\n",
    "                    remove_components=15,\n",
    "                    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in training dataset: 5749\n",
      "Samples in test dataset 1379\n",
      "Fitting on training dataset\n",
      "Computing similarity scores on test dataset\n",
      "Pearson`s r (x 100) on test dataset: 71.6725415272\n"
     ]
    }
   ],
   "source": [
    "evaluate_vectorizer(train=train, test=test,\n",
    "                    word_vectorizer=lambda w: nlp.vocab[w].vector,\n",
    "                    tokenizer=lambda s: [t.text for t in nlp.tokenizer(s) if not t.is_punct and not t.is_space],\n",
    "                    word_freq=lambda w: word_freq.get(w, 0.0),\n",
    "                    weighted=True,\n",
    "                    remove_components=15,\n",
    "                    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}